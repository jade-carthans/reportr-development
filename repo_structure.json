{
  "README.md": {
    "content": "# Reportr - Git Repository Progress Reporter\n\nA Python tool that uses Azure OpenAI to generate comprehensive progress reports for git repositories by analyzing commit history, contributors, and code changes.\n\n## Features\n\n- **Git History Analysis**: Analyzes commit history, contributors, and code changes\n- **Progress Reports**: Generates professional progress reports using AI\n- **Flexible Time Periods**: Can analyze recent commits or entire repository history\n- **Contributor Statistics**: Tracks commits, lines added/deleted per contributor\n- **Multiple Output Modes**: Progress reports or simple summaries\n- **Modular Architecture**: Features are separated into individual modules for easy maintenance\n\n## Project Structure\n\n``` txt\nreportr/\n├── reportr_client.py          # Main client application\n├── requirements.txt           # Python dependencies\n├── README.md                  # This file\n├── features/                  # Feature modules\n│   ├── progress-report/       # Git progress report feature\n│   │   ├── progress-report.py # Progress report implementation\n│   │   └── prompt.txt         # AI prompt template\n│   └── summarize-repo/        # Repository summary feature\n│       ├── summarize_repo.py  # Summary implementation\n│       └── prompt.txt         # AI prompt template\n└── venv/                      # Virtual environment\n```\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Set up your environment variables in a `.env` file:\n\n   ```\n   AZURE_OPENAI_KEY=your_azure_openai_key_here\n   ```\n\n## Usage\n\n### Basic Progress Report\n\nGenerate a progress report for the current repository (last 30 days):\n\n```bash\npython reportr_client.py\n```\n\n### Custom Time Period\n\nGenerate a report for the last 7 days:\n\n```bash\npython reportr_client.py --days 7\n```\n\nGenerate a report for all time:\n\n```bash\npython reportr_client.py --days 0\n```\n\n### Different Repository Path\n\nAnalyze a different repository:\n\n```bash\npython reportr_client.py --repo-path /path/to/other/repo\n```\n\n### Simple Summary Mode\n\nGet a simple repository summary instead of a progress report:\n\n```bash\npython reportr_client.py --mode summary\n```\n\n## Command Line Options\n\n- `--repo-path`: Path to the git repository (default: current directory)\n- `--days`: Number of days to look back (default: 30, use 0 for all time)\n- `--mode`: Mode selection - 'progress' or 'summary' (default: progress)\n\n## Output\n\nThe progress report includes:\n\n- Executive summary of development activity\n- Key contributors and their contributions\n- Major changes and improvements made\n- Development patterns and trends\n- Summary of code changes (additions/deletions)\n\n## Architecture\n\nThe application uses a modular architecture where:\n\n- **Main Client** (`reportr_client.py`): Handles command-line interface and orchestrates features\n- **Feature Modules**: Each feature is in its own directory with implementation and prompt files\n- **Client Injection**: The Azure OpenAI client is injected into each feature function for better testability and modularity\n- **Prompt Templates**: AI prompts are stored in separate `.txt` files for easy customization\n\n## Requirements\n\n- Python 3.7+\n- Git repository with commit history\n- Azure OpenAI API access\n- Internet connection for API calls\n"
  },
  "features": {
    "__init__.py": {
      "content": "# Features package \nfrom features.progress_report.progress_report import create_progress_report, get_git_history\nfrom features.summarize_repo.summarize_repo import summarize_repo\nfrom features.generate_readme.generate_readme import generate_readme, analyze_repository_structure\n\n__all__ = ['create_progress_report', 'get_git_history', 'summarize_repo', 'generate_readme', 'analyze_repository_structure'] "
    },
    "generate_readme": {
      "__init__.py": {
        "content": "from .generate_readme import generate_readme, analyze_repository_structure\n\n__all__ = ['generate_readme', 'analyze_repository_structure'] "
      },
      "generate_readme.py": {
        "content": "import os\nimport json\nimport openai\nfrom openai import AzureOpenAI\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\ndef analyze_repository_structure(repo_path=\".\"):\n    \"\"\"\n    Analyze the repository structure to understand the project type and content\n    \"\"\"\n    repo_path = Path(repo_path)\n    analysis = {\n        'repo_name': repo_path.name,\n        'files': [],\n        'directories': [],\n        'file_extensions': {},\n        'has_requirements': False,\n        'has_package_json': False,\n        'has_dockerfile': False,\n        'has_makefile': False,\n        'has_readme': False,\n        'has_license': False,\n        'has_tests': False,\n        'has_docs': False,\n        'main_language': None,\n        'project_type': 'unknown'\n    }\n    \n    # Common file patterns to look for\n    important_files = [\n        'requirements.txt', 'package.json', 'Dockerfile', 'Makefile', \n        'README.md', 'README.txt', 'LICENSE', 'LICENSE.txt', 'setup.py',\n        'pyproject.toml', 'Cargo.toml', 'go.mod', 'pom.xml', 'build.gradle',\n        '.gitignore', 'docker-compose.yml', 'docker-compose.yaml'\n    ]\n    \n    # Walk through the repository\n    for root, dirs, files in os.walk(repo_path):\n        # Skip hidden directories and common ignore patterns\n        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules', 'venv', 'env', '.git']]\n        \n        rel_root = Path(root).relative_to(repo_path)\n        \n        for file in files:\n            if file.startswith('.'):\n                continue\n                \n            file_path = rel_root / file\n            analysis['files'].append(str(file_path))\n            \n            # Check file extension\n            ext = file_path.suffix.lower()\n            if ext:\n                analysis['file_extensions'][ext] = analysis['file_extensions'].get(ext, 0) + 1\n            \n            # Check for important files\n            if file.lower() in [f.lower() for f in important_files]:\n                if file.lower() in ['requirements.txt', 'pyproject.toml', 'setup.py']:\n                    analysis['has_requirements'] = True\n                elif file.lower() == 'package.json':\n                    analysis['has_package_json'] = True\n                elif file.lower() == 'dockerfile':\n                    analysis['has_dockerfile'] = True\n                elif file.lower() == 'makefile':\n                    analysis['has_makefile'] = True\n                elif file.lower().startswith('readme'):\n                    analysis['has_readme'] = True\n                elif file.lower().startswith('license'):\n                    analysis['has_license'] = True\n        \n        # Check for test directories\n        if any(test_dir in str(rel_root).lower() for test_dir in ['test', 'tests', 'spec', 'specs']):\n            analysis['has_tests'] = True\n        \n        # Check for documentation directories\n        if any(doc_dir in str(rel_root).lower() for doc_dir in ['doc', 'docs', 'documentation']):\n            analysis['has_docs'] = True\n    \n    # Determine main language and project type\n    if analysis['file_extensions']:\n        main_ext = max(analysis['file_extensions'].items(), key=lambda x: x[1])[0]\n        analysis['main_language'] = main_ext\n        \n        if main_ext in ['.py', '.pyx']:\n            analysis['project_type'] = 'python'\n        elif main_ext in ['.js', '.jsx', '.ts', '.tsx']:\n            analysis['project_type'] = 'javascript'\n        elif main_ext in ['.java']:\n            analysis['project_type'] = 'java'\n        elif main_ext in ['.go']:\n            analysis['project_type'] = 'go'\n        elif main_ext in ['.rs']:\n            analysis['project_type'] = 'rust'\n        elif main_ext in ['.cpp', '.c', '.h', '.hpp']:\n            analysis['project_type'] = 'cpp'\n        elif main_ext in ['.cs']:\n            analysis['project_type'] = 'csharp'\n        elif main_ext in ['.php']:\n            analysis['project_type'] = 'php'\n        elif main_ext in ['.rb']:\n            analysis['project_type'] = 'ruby'\n        elif main_ext in ['.swift']:\n            analysis['project_type'] = 'swift'\n        elif main_ext in ['.kt']:\n            analysis['project_type'] = 'kotlin'\n    \n    return analysis\n\ndef generate_readme(client, repo_path=\".\"):\n    \"\"\"\n    Generate a comprehensive README file for a repository based on its structure and content\n    \"\"\"\n    print(\"Analyzing repository structure...\")\n    repo_analysis = analyze_repository_structure(repo_path)\n    \n    # Prepare the analysis data for the LLM\n    analysis_context = f\"\"\"\nRepository Analysis:\nName: {repo_analysis['repo_name']}\nProject Type: {repo_analysis['project_type']}\nMain Language: {repo_analysis['main_language']}\n\nRepository Structure:\n- Total Files: {len(repo_analysis['files'])}\n- File Extensions: {dict(list(repo_analysis['file_extensions'].items())[:10])}  # Top 10 extensions\n\nKey Files Present:\n- Requirements/Dependencies: {repo_analysis['has_requirements']}\n- Package Configuration: {repo_analysis['has_package_json']}\n- Docker Support: {repo_analysis['has_dockerfile']}\n- Build System: {repo_analysis['has_makefile']}\n- Existing README: {repo_analysis['has_readme']}\n- License: {repo_analysis['has_license']}\n- Tests: {repo_analysis['has_tests']}\n- Documentation: {repo_analysis['has_docs']}\n\nFiles in Repository:\n{chr(10).join(repo_analysis['files'][:50])}  # First 50 files\n\"\"\"\n    \n    # Load the prompt template\n    prompt_path = os.path.join(os.path.dirname(__file__), 'prompt.txt')\n    try:\n        with open(prompt_path, 'r') as f:\n            prompt_template = f.read()\n        \n        # Replace the placeholder with actual data\n        prompt_template = prompt_template.replace('{analysis_context}', analysis_context)\n        messages = json.loads(prompt_template)\n        \n    except Exception as e:\n        print(f\"Error loading prompt template: {e}\")\n        # Fallback to hardcoded prompt\n        messages = [\n            {\n                \"role\": \"system\", \n                \"content\": \"\"\"You are a helpful assistant that generates comprehensive, professional README files for software projects. \n                Analyze the repository structure and create a README that includes:\n                1. Project title and description\n                2. Features and capabilities\n                3. Installation instructions\n                4. Usage examples\n                5. Configuration options\n                6. Contributing guidelines\n                7. License information\n                8. Any other relevant sections based on the project type\n                \n                Write in clear, professional markdown format suitable for GitHub or similar platforms.\n                Make the README engaging and informative for potential users and contributors.\"\"\"\n            },\n            {\n                \"role\": \"user\", \n                \"content\": f\"Generate a comprehensive README file for this repository based on the following analysis:\\n\\n{analysis_context}\"\n            }\n        ]\n    \n    # Generate the README using the LLM\n    response = client.chat.completions.create(\n        model=\"reportr\",\n        messages=messages,\n        max_tokens=3000,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content \n\ndef write_to_readme_file(readme_content, output_path=\"TEST_README.md\"):\n    \"\"\"\n    Write the generated README content to a file\n    \"\"\"\n    try:\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(readme_content)\n        print(f\"README file written successfully to {output_path}\")\n        return True\n    except Exception as e:\n        print(f\"Error writing README file: {e}\")\n        return False "
      },
      "prompt.txt": {
        "content": "[\n    {\n        \"role\": \"system\", \n        \"content\": \"You are a helpful assistant that generates comprehensive, professional README files for software projects. Analyze the repository structure and create a README that includes: 1. Project title and description 2. Features and capabilities 3. Installation instructions 4. Usage examples 5. Configuration options 6. Contributing guidelines 7. License information 8. Any other relevant sections based on the project type. Write in clear, professional markdown format suitable for GitHub or similar platforms. Make the README engaging and informative for potential users and contributors. Based on the project type, include appropriate sections: - For Python projects: pip install instructions, virtual environment setup - For JavaScript/Node.js: npm/yarn install instructions - For Docker projects: docker run commands - For Go projects: go install/get commands - For Rust projects: cargo commands - For Java projects: maven/gradle commands. Always include badges for build status, version, license if applicable. Make the README visually appealing with proper markdown formatting.\"\n    },\n    {\n        \"role\": \"user\", \n        \"content\": \"Generate a comprehensive README file for this repository based on the following analysis:\\n\\n{analysis_context}\"\n    }\n] "
      }
    },
    "progress_report": {
      "__init__.py": {
        "content": "from .progress_report import create_progress_report, get_git_history\n\n__all__ = ['create_progress_report', 'get_git_history'] "
      },
      "progress_report.py": {
        "content": "import os\nimport json\nfrom git import Repo\nfrom datetime import datetime, timedelta\n\n\ndef get_git_history(repo_path=\".\", days_back=30, contributor_filter=None):\n    \"\"\"\n    Extract git history information from the repository\n    Args:\n        repo_path: Path to the repository\n        days_back: Number of days to look back (0 for all time)\n        contributor_filter: Optional list of contributor names to filter by\n    \"\"\"\n    try:\n        repo = Repo(repo_path)\n\n        # Get commits from the last N days\n        since_date = datetime.now() - timedelta(days=days_back)\n        commits = list(repo.iter_commits(\"main\", since=since_date))\n\n        if not commits:\n            # If no commits in main, try master branch\n            commits = list(repo.iter_commits(\"master\", since=since_date))\n\n        if not commits:\n            # If still no commits, get all commits\n            commits = list(repo.iter_commits())\n\n        # Collect commit information\n        commit_data = []\n        contributors = {}\n\n        for commit in commits:\n            # Skip merge commits for cleaner analysis\n            if len(commit.parents) > 1:\n                continue\n\n            author_name = commit.author.name\n            author_email = commit.author.email\n\n            # Filter by contributor if specified\n            if contributor_filter and author_name not in contributor_filter:\n                continue\n\n            commit_date = datetime.fromtimestamp(commit.committed_date)\n            commit_message = commit.message.strip()\n\n            # Count contributions per author\n            if author_name not in contributors:\n                contributors[author_name] = {\n                    \"email\": author_email,\n                    \"commits\": 0,\n                    \"lines_added\": 0,\n                    \"lines_deleted\": 0,\n                    \"files_changed\": 0,\n                    \"commit_messages\": [],\n                }\n\n            contributors[author_name][\"commits\"] += 1\n            contributors[author_name][\"commit_messages\"].append(commit_message)\n\n            # Get stats for this commit\n            lines_added = 0\n            lines_deleted = 0\n            files_changed = 0\n            try:\n                stats = commit.stats\n                lines_added = stats.total[\"insertions\"]\n                lines_deleted = stats.total[\"deletions\"]\n                files_changed = len(stats.files)\n                contributors[author_name][\"lines_added\"] += lines_added\n                contributors[author_name][\"lines_deleted\"] += lines_deleted\n                contributors[author_name][\"files_changed\"] += files_changed\n            except:\n                pass\n\n            commit_data.append(\n                {\n                    \"hash\": commit.hexsha[:8],\n                    \"author\": author_name,\n                    \"date\": commit_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                    \"message\": commit_message,\n                    \"lines_added\": lines_added,\n                    \"lines_deleted\": lines_deleted,\n                    \"files_changed\": files_changed,\n                }\n            )\n\n        return {\n            \"repo_name\": (\n                str(repo.working_dir).split(\"/\")[-1] if repo.working_dir else \"Unknown\"\n            ),\n            \"total_commits\": len(commit_data),\n            \"contributors\": contributors,\n            \"commits\": commit_data,\n            \"period\": f\"Last {days_back} days\" if days_back > 0 else \"All time\",\n            \"filtered_by\": (\n                contributor_filter if contributor_filter else \"All contributors\"\n            ),\n        }\n\n    except Exception as e:\n        print(f\"Error accessing git repository: {e}\")\n        return None\n\n\ndef create_contributor_summary(client, git_data, contributor_name):\n    \"\"\"\n    Create a detailed summary for a specific contributor\n    \"\"\"\n    if contributor_name not in git_data[\"contributors\"]:\n        return f\"Contributor '{contributor_name}' not found in the repository.\"\n\n    contributor_data = git_data[\"contributors\"][contributor_name]\n    contributor_commits = [\n        c for c in git_data[\"commits\"] if c[\"author\"] == contributor_name\n    ]\n\n    summary_context = f\"\"\"\n    Contributor Analysis: {contributor_name}\n    Email: {contributor_data['email']}\n    Period: {git_data['period']}\n    \n    Summary Statistics:\n    - Total Commits: {contributor_data['commits']}\n    - Lines Added: {contributor_data['lines_added']}\n    - Lines Deleted: {contributor_data['lines_deleted']}\n    - Files Changed: {contributor_data['files_changed']}\n    - Net Lines: {contributor_data['lines_added'] - contributor_data['lines_deleted']}\n    \n    Recent Commit Messages:\n    \"\"\"\n\n    for commit in contributor_commits[:15]:  # Show last 15 commits\n        summary_context += f\"\"\"\n    - {commit['date']} ({commit['hash']})\n      {commit['message']}\n      +{commit['lines_added']} -{commit['lines_deleted']} lines, {commit['files_changed']} files\n    \"\"\"\n\n    # Create a specialized prompt for contributor analysis\n    contributor_prompt = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert at analyzing developer contributions. Create a detailed, professional summary of a contributor's work that includes: 1. Overall contribution assessment 2. Development patterns and focus areas 3. Code quality indicators (based on commit patterns) 4. Key achievements and notable changes 5. Recommendations or observations about their work style. Write in clear, professional language.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Analyze this contributor's work and create a detailed summary:\\n\\n{summary_context}\",\n        },\n    ]\n\n    response = client.chat.completions.create(\n        model=\"reportr\", messages=contributor_prompt, max_tokens=1500, temperature=0.7\n    )\n\n    return response.choices[0].message.content\n\n\ndef create_progress_report(\n    client,\n    repo_path=\".\",\n    days_back=30,\n    contributor_filter=None,\n    include_contributor_summaries=False,\n):\n    \"\"\"\n    Create a comprehensive progress report for a git repository\n    Args:\n        client: The LLM client\n        repo_path: Path to the repository\n        days_back: Number of days to look back (0 for all time)\n        contributor_filter: Optional list of contributor names to filter by\n        include_contributor_summaries: Whether to include detailed summaries for each contributor\n    \"\"\"\n    print(\"Analyzing git repository...\")\n    git_data = get_git_history(repo_path, days_back, contributor_filter)\n\n    if not git_data:\n        print(\"Could not analyze git repository. Make sure you're in a git repository.\")\n        return\n\n    # Prepare the data for the LLM\n    report_context = f\"\"\"\n        Repository: {git_data['repo_name']}\n        Analysis Period: {git_data['period']}\n        Filter: {git_data['filtered_by']}\n        Total Commits: {git_data['total_commits']}\n\n        Contributors ({len(git_data['contributors'])}):\n    \"\"\"\n\n    for author, stats in git_data[\"contributors\"].items():\n        report_context += f\"\"\"\n        - {author} ({stats['email']})\n        - Commits: {stats['commits']}\n        - Lines Added: {stats['lines_added']}\n        - Lines Deleted: {stats['lines_deleted']}\n        - Files Changed: {stats['files_changed']}\n        - Net Lines: {stats['lines_added'] - stats['lines_deleted']}\n        \"\"\"\n\n    report_context += \"\\nRecent Commits:\\n\"\n    for commit in git_data[\"commits\"][:20]:  # Show last 20 commits\n        report_context += f\"\"\"\n- {commit['date']} - {commit['author']} ({commit['hash']})\n  {commit['message']}\n  +{commit['lines_added']} -{commit['lines_deleted']} lines, {commit['files_changed']} files\n\"\"\"\n\n    # Load the prompt template\n    prompt_path = os.path.join(os.path.dirname(__file__), \"prompt.txt\")\n    messages = None\n    try:\n        with open(prompt_path, \"r\") as f:\n            prompt_template = f.read()\n\n        # Escape the report_context to make it JSON-safe\n        escaped_context = json.dumps(report_context)\n        # Remove the outer quotes since we're inserting into a string\n        escaped_context = escaped_context[1:-1]\n\n        # Replace the placeholder with actual data\n        prompt_template = prompt_template.replace(\"{report_context}\", escaped_context)\n        messages = json.loads(prompt_template)\n    except Exception as e:\n        print(f\"Error loading prompt template: {e}\")\n        return\n\n    # Generate the main report using the LLM\n    response = client.chat.completions.create(\n        model=\"reportr\", messages=messages, max_tokens=2000, temperature=0.7\n    )\n\n    main_report = response.choices[0].message.content\n\n    # Add contributor summaries if requested\n    if include_contributor_summaries and git_data[\"contributors\"]:\n        main_report += (\n            \"\\n\\n\" + \"=\" * 50 + \"\\nDETAILED CONTRIBUTOR SUMMARIES\\n\" + \"=\" * 50 + \"\\n\"\n        )\n\n        for contributor_name in git_data[\"contributors\"].keys():\n            contributor_summary = create_contributor_summary(\n                client, git_data, contributor_name\n            )\n            main_report += f\"\\n\\n{contributor_summary}\\n\"\n            main_report += \"-\" * 50\n\n    return main_report\n"
      },
      "prompt.txt": {
        "content": "[\n    {\n        \"role\": \"system\", \n        \"content\": \"You are a helpful assistant that creates clear, professional progress reports for software development projects. Analyze the git history data provided and create a comprehensive progress report that includes: 1. Executive summary of development activity 2. Key contributors and their contributions 3. Major changes and improvements made 4. Development patterns and trends 5. Summary of code changes (additions/deletions). Write in clear, professional language suitable for stakeholders and team members.\"\n    },\n    {\n        \"role\": \"user\", \n        \"content\": \"Create a progress report for this repository based on the following git history data:\\n\\n{report_context}\"\n    }\n] "
      }
    },
    "summarize_repo": {
      "__init__.py": {
        "content": "from .summarize_repo import summarize_repo\n\n__all__ = ['summarize_repo'] "
      },
      "prompt.txt": {
        "content": "[\n  {\n    \"role\": \"system\",\n    \"content\": \"You are a precise and confident assistant that deeply understands Python codebases and summarizes them for onboarding engineers.\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"You are analyzing the directory:\\n\\n📁 {path}\\n\\nIt contains the following Python files and their contents:\\n\\n{file_contents}\\n\\nPlease read and understand the actual code in each file. Then write a clear, confident 2-3 sentence summary of what this directory does. Avoid vague language like 'likely' or 'probably'. Focus on the real functionality and how the files work together.\"\n  }\n]"
      },
      "summarize_repo.py": {
        "content": "import os\nimport json\nimport argparse\nfrom openai import AzureOpenAI\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n# Set up Azure OpenAI client\nclient = AzureOpenAI(\n    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n    api_version=\"2024-02-15-preview\",\n    azure_endpoint=\"https://natalie-design-agent-resource.cognitiveservices.azure.com/\",\n)\n\n\n# Walk the repo and collect relevant files by directory\ndef collect_relevant_files(repo_path):\n    directory_map = {}\n    SKIP_DIRS = {\"venv\", \".git\", \"__pycache__\"}\n    INCLUDED_EXTENSIONS = {\".py\", \".md\", \".txt\"}\n\n    for root, dirs, files in os.walk(repo_path):\n        # Skip hidden and irrelevant directories\n        dirs[:] = [d for d in dirs if d not in SKIP_DIRS and not d.startswith(\".\")]\n        relevant_files = [\n            f for f in files if os.path.splitext(f)[1] in INCLUDED_EXTENSIONS\n        ]\n        if relevant_files:\n            directory_map[root] = relevant_files\n    return directory_map\n\n\n# Load prompt template from prompt.txt and inject file contents\ndef load_prompt_template(path, file_contents):\n    prompt_path = os.path.join(os.path.dirname(__file__), \"prompt.txt\")\n    with open(prompt_path, \"r\") as f:\n        prompt_template = json.load(f)\n\n    # Replace placeholders\n    for message in prompt_template:\n        if \"{path}\" in message[\"content\"]:\n            message[\"content\"] = message[\"content\"].replace(\"{path}\", path)\n        if \"{file_contents}\" in message[\"content\"]:\n            message[\"content\"] = message[\"content\"].replace(\n                \"{file_contents}\", file_contents\n            )\n    return prompt_template\n\n\n# Summarize a directory using the model\ndef summarize_directory(path, files, client):\n    file_contents = \"\"\n    for file in files:\n        file_path = os.path.join(path, file)\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            file_contents += f\"\\n\\n📄 File: {file}\\n{content}\"\n        except Exception as e:\n            file_contents += f\"\\n\\n📄 File: {file}\\n# Error reading file: {e}\"\n\n    messages = load_prompt_template(path, file_contents)\n    response = client.chat.completions.create(model=\"reportr\", messages=messages)\n    # print json dumps of prompt\n    # print(json.dumps(messages, indent=2))\n    return response.choices[0].message.content\n\n\n# Print directory tree\ndef print_tree(root_path, prefix=\"\"):\n    SKIP_DIRS = {\"venv\", \".git\", \"__pycache__\"}\n\n    for item in sorted(os.listdir(root_path)):\n        full_path = os.path.join(root_path, item)\n        if os.path.isdir(full_path):\n            if item in SKIP_DIRS or item.startswith(\".\"):\n                continue\n            print(f\"{prefix}📁 {item}\")\n            # print_tree(full_path, prefix + \"    \")\n        elif os.path.isfile(full_path):\n            print(f\"{prefix}📄 {item}\")\n\n\n# Main function to be called by the client\ndef summarize_repo(client, repo_path=\".\"):\n    \"\"\"\n    Summarize the repository structure and contents.\n\n    Args:\n        client: Azure OpenAI client instance\n        repo_path: Path to the repository (default: current directory)\n\n    Returns:\n        str: Summary of the repository\n    \"\"\"\n    summary_parts = []\n\n    # Add directory tree\n    summary_parts.append(\"📂 Directory Tree:\")\n    tree_output = []\n\n    def capture_tree(root_path, prefix=\"\"):\n        SKIP_DIRS = {\"venv\", \".git\", \"__pycache__\"}\n\n        for item in sorted(os.listdir(root_path)):\n            full_path = os.path.join(root_path, item)\n\n            print(f\"Processing item: {item} at path: {full_path}\")\n\n            if os.path.isdir(full_path):\n                if item in SKIP_DIRS or item.startswith(\".\"):\n                    continue\n                tree_output.append(f\"{prefix}📁 {item}\")\n                capture_tree(full_path, prefix + \"    \")\n            elif os.path.isfile(full_path):\n                tree_output.append(f\"{prefix}📄 {item}\")\n\n            # print(tree_output)\n\n    capture_tree(repo_path)\n    summary_parts.append(\"\\n\".join(tree_output))\n\n    # Add summaries for each directory\n    summary_parts.append(\"\\n🧠 Summaries:\")\n    directory_map = collect_relevant_files(repo_path)\n\n    for path, files in directory_map.items():\n        summary_parts.append(f\"\\n📁 Directory: {path}\")\n        summary = summarize_directory(path, files, client)\n        summary_parts.append(f\"📝 Summary:\\n{summary}\")\n\n    return \"\\n\".join(summary_parts)\n\n\n# Build nested dictionary structure of repository\ndef build_repo_structure(repo_path):\n    \"\"\"\n    Build a nested dictionary structure representing the repository.\n\n    Args:\n        repo_path: Path to the repository\n\n    Returns:\n        dict: Nested dictionary with folder names as keys and lists of file dictionaries as values\n    \"\"\"\n    SKIP_DIRS = {\"venv\", \".git\", \"__pycache__\"}\n    INCLUDED_EXTENSIONS = {\".py\", \".md\", \".txt\"}\n\n    def _build_structure(path):\n        structure = {}\n\n        for item in sorted(os.listdir(path)):\n            full_path = os.path.join(path, item)\n\n            if os.path.isdir(full_path):\n                if item in SKIP_DIRS or item.startswith(\".\"):\n                    continue\n                # Recursively build structure for subdirectories\n                structure[item] = _build_structure(full_path)\n            elif os.path.isfile(full_path):\n                # Only include files with specified extensions\n                if os.path.splitext(item)[1] in INCLUDED_EXTENSIONS:\n                    try:\n                        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        structure[item] = {\"content\": content}\n                    except Exception as e:\n                        structure[item] = {\"content\": f\"# Error reading file: {e}\"}\n\n        return structure\n\n    return _build_structure(repo_path)\n\n\ndef save_repo_structure_to_json(repo_path, output_file=\"repo_structure.json\"):\n    \"\"\"\n    Build a nested dictionary structure representing the repository and save it to a JSON file.\n\n    Args:\n        repo_path: Path to the repository\n        output_file: Name of the output JSON file (default: repo_structure.json)\n\n    Returns:\n        str: Path to the created JSON file\n    \"\"\"\n    # Build the structure\n    structure = build_repo_structure(repo_path)\n\n    # Save to JSON file\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(structure, f, indent=2, ensure_ascii=False)\n\n    return output_file\n"
      }
    }
  },
  "reportr_client.py": {
    "content": "import os\nimport argparse\nfrom openai import AzureOpenAI\nfrom dotenv import load_dotenv\nfrom features.progress_report.progress_report import create_progress_report\nfrom features.generate_readme.generate_readme import (\n    generate_readme,\n    write_to_readme_file,\n)\nfrom features.summarize_repo.summarize_repo import (\n    summarize_repo,\n    save_repo_structure_to_json,\n)\n\nload_dotenv()\n\n\n# create the azure openai client\ndef create_client():\n    \"\"\"Create and return an Azure OpenAI client\"\"\"\n\n    return AzureOpenAI(\n        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n        api_version=\"2024-02-15-preview\",\n        azure_endpoint=\"https://natalie-design-agent-resource.cognitiveservices.azure.com/\",\n    )\n\n\n# parse the arguments from the command line\ndef parse_arguments():\n    \"\"\"Parse and return command line arguments\"\"\"\n\n    # create the parser\n    parser = argparse.ArgumentParser(\n        description=\"Reportr - AI-powered repository analysis and documentation tool\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\n            Examples:\n            python reportr_client.py --progress-report\n            python reportr_client.py --generate-readme\n            python reportr_client.py --summarize-repo --path /path/to/repo\n        \"\"\",\n    )\n\n    # create subparsers for different commands\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # progress-report subcommand\n    progress_parser = subparsers.add_parser(\n        \"progress-report\", help=\"Generate a progress report for the current repository\"\n    )\n    progress_parser.add_argument(\n        \"--username\",\n        action=\"append\",\n        help=\"Filter by specific contributor username(s). Can be used multiple times.\",\n    )\n    progress_parser.add_argument(\n        \"--days\",\n        type=int,\n        default=30,\n        help=\"Number of days to look back (default: 30, use 0 for all time)\",\n    )\n    progress_parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed contributor summaries\"\n    )\n\n    # generate-readme subcommand\n    readme_parser = subparsers.add_parser(\n        \"generate-readme\", help=\"Generate a README file for the current repository\"\n    )\n\n    # 'summarize-repo' arg to summarize the purpose of the current repository\n    parser.add_argument(\n        \"--summarize-repo\",\n        action=\"store_true\",\n        help=\"Summarize the purpose of the current repository\",\n    )\n\n    # 'path' argument to specify the local path to the repository or directory\n    parser.add_argument(\n        \"--path\",\n        type=str,\n        default=\".\",\n        help=\"Path to the local repository or directory to summarize (default: current directory)\",\n    )\n\n    # 'save-json' argument to save repository structure to JSON file\n    parser.add_argument(\n        \"--save-json\",\n        type=str,\n        help=\"Save repository structure to JSON file (specify filename or use default: repo_structure.json)\",\n        nargs=\"?\",\n        const=\"repo_structure.json\",\n    )\n\n    return parser.parse_args()\n\n\n# execute the features based on the provided arguments\ndef execute_features(args):\n    \"\"\"Execute the requested features based on parsed arguments\"\"\"\n\n    # create the client\n    client = create_client()\n\n    results = []\n\n    # if 'progress-report' command is provided, generate a progress report\n    if args.command == \"progress-report\":\n        report = create_progress_report(\n            client,\n            days_back=args.days,\n            contributor_filter=args.username,\n            include_contributor_summaries=args.detailed,\n        )\n        results.append((\"Progress Report\", report))\n\n    # if 'generate-readme' command is provided, generate a README file\n    elif args.command == \"generate-readme\":\n        readme = generate_readme(client)\n        write_to_readme_file(readme)\n        results.append((\"README\", readme))\n\n    # if 'summarize-repo' is provided, summarize the purpose of the current repository\n    if args.summarize_repo:\n        summary = summarize_repo(client, repo_path=args.path)\n        results.append((\"Repository Summary\", summary))\n\n    # if 'save-json' is provided, save repository structure to JSON file\n    if args.save_json:\n        output_file = save_repo_structure_to_json(args.path, args.save_json)\n        results.append(\n            (\"JSON Structure\", f\"Repository structure saved to: {output_file}\")\n        )\n\n    return results\n\n\ndef main():\n    \"\"\"Main function to handle CLI arguments and execute features\"\"\"\n\n    # parse the arguments\n    args = parse_arguments()\n\n    # if no command provided but summarize-repo flag is used, execute it\n    if not args.command and (args.summarize_repo or args.save_json):\n        results = execute_features(args)\n        # print the results\n        for title, content in results:\n            print(f\"{title.upper()}\\n\\n{content}\\n\\n\\n\\n\")\n        return\n\n    # if no command provided, show help\n    if not args.command:\n        parser = argparse.ArgumentParser(\n            description=\"Reportr - AI-powered repository analysis and documentation tool\"\n        )\n        parser.print_help()\n        return\n\n    # execute the requested features\n    results = execute_features(args)\n\n    # print the results\n    for title, content in results:\n        print(f\"{title.upper()}\\n\\n{content}\\n\\n\\n\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "requirements.txt": {
    "content": "annotated-types==0.7.0\nanyio==4.9.0\ncertifi==2025.6.15\ncharset-normalizer==3.4.2\ndistro==1.9.0\ngitpython==3.1.44\nh11==0.16.0\nhttpcore==1.0.9\nhttpx==0.28.1\nidna==3.10\njiter==0.10.0\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain-core==0.3.66\nlangsmith==0.4.1\nopenai==1.91.0\norjson==3.10.18\npackaging==24.2\npydantic==2.11.7\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsniffio==1.3.1\ntenacity==9.1.2\ntqdm==4.67.1\ntyping-inspection==0.4.1\ntyping_extensions==4.14.0\nurllib3==2.5.0\nzstandard==0.23.0\n"
  }
}